{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5756,"status":"ok","timestamp":1682539836347,"user":{"displayName":"Weishan Zhang","userId":"02899996824368835811"},"user_tz":240},"id":"WQhOO-fm0zHk","outputId":"ee133e00-b28a-4485-83c1-52c3820147cf","scrolled":true},"outputs":[],"source":["'''2024 Feb version. model pre-trained on normal 2xdata, tested with PUs using Random Mod type \n","CEL vs Energy detection, 2022 Mar 14 Trying on Colab. \n","It seems that processing small batch is slow, 7s per epoch per user\n","Big batch can accelerate to less than 2s per epoch per user\n","Copied on 2023Feb3'''\n","import torch\n","import os\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","import math\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.utils.data.distributed as TUDdistributed\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from torch.autograd import Variable\n","import sys\n","from copy import deepcopy\n","import random\n","import collections\n","#from MobileNetV1_CriticalPath import Net\n","import matplotlib.pyplot as plt\n","import pandas as pd \n","# from sklearn.externals import joblib\n","from cnn_models import standalone_cnn\n","from cnn_models import decouple_cnn\n","from cnn_models import decouple_cnn_mod\n","\n","from pytz import timezone\n","TMZ = timezone('EST')\n","import datetime\n","import shutil\n","import time\n","\n","device = torch.device(\"cuda\")\n","# if os.environ[\"CUDA_VISIBLE_DEVICES\"]:\n","#     device = torch.device(\"cuda\")\n","# else:\n","#     device = torch.device(\"cpu\")\n","\n","# device = torch.device(\"cpu\")\n","use_cuda = True\n","criterion = nn.CrossEntropyLoss()\n","\n","random.seed(0) \n","\n","      \n","\n","def create_net(list, per_class_filter, ty_chs):\n","    number_class = len(list) # list of locally observable channels\n","    d_w = number_class*per_class_filter\n","    cfg = [40, d_w, d_w, d_w ]\n","    print('corresponding cfg channel list:',cfg)\n","    net = decouple_cnn_mod( nch=number_class, cfg=cfg, ty_chs=ty_chs)\n","    net.to(device)\n","    return net\n","\n","def testnetsVote(model_list, class_dir, test_loader, coef_list, gain_dif, thresh_sig = 0.5):                             \n","    '''default for FL CEL stdaln, Tests all nodes together, apply majority vote for each band \n","    '''\n","    total = 0 #sum of occupation and emptiness\n","    test_loss = 0 \n","    total_ocp = 0 #occupied bands\n","    total_emp = 0 #empty bands\n","    correct = 0\n","    total_cmb = 0  #occupation combination\n","    correct_ocp = 0\n","    correct_emp = 0\n","    correct_cmb = 0\n","    thresh_logit = -1*(math.log(thresh_sig**(-1) -1))\n","    # thresh_logit = thresh_sig\n","    \n","    with torch.no_grad():\n","        criterion = nn.BCEWithLogitsLoss()\n","        for batchidx, (data, target) in enumerate(test_loader):\n","            target =  target.cuda()\n","            #print(target.size())\n","            target = Variable(target)\n","            target = Variable(target)\n","#             print('target size is:',target.size())\n","            output_manual = torch.zeros(target.shape) # manual global decision, float version\n","            total_batch = float(torch.tensor(target.size()).prod()) #total num of channels in this batch\n","            total += total_batch\n","            total_cmb += float(target.size(0))\n","            total_ocp += float(target.sum())\n","            total_emp += float(total_batch - target.sum()) \n","            if (1-target).sum() != (torch.tensor(target.size()).prod() - target.sum()) : #Testing tensor dim =========\n","                print('show difference:',(1-target).sum(), total - target.sum())\n","                print('total ocp/emp calculation wrong')\n","            '''Above calculate the total number of channels detected and the ground truth number of occupation/emptiness'''\n","            coef = torch.tensor(coef_list)\n","            for idx in range(len(class_dir)): # appply fusion strategy on local_node detections\n","                model_list[idx].eval()\n","                datain = Variable(gain_dif*1e7*data[idx]).cuda() #1e7 to boost gradient\n","                localout = (model_list[idx](datain).cpu()>thresh_logit).float()\n","                output_manual[:,class_dir[idx]] = output_manual[:,class_dir[idx]].add_(localout) #add local detection result\n","                # output_manual[:,class_dir[idx]] = output_manual[:,class_dir[idx]]|(model_list[idx](datain).cpu()>thresh_logit)\n","            out_WOnorm = output_manual\n","            output_manual = output_manual.div_( coef ) > 0.49 #get fusion decision\n","            if batchidx == -1 :\n","                id = 90\n","                print('label:',target[id])\n","                print('un-normalized fusion:', torch.norm(out_WOnorm))\n","                print('SU fusion:',output_manual[id])\n","\n","            correct_ocp += (output_manual*target.cpu()).sum()\n","            correct_emp += ((~output_manual)*(1-target.cpu())).sum()\n","            correct_cmb += (~(output_manual^( target.bool().cpu() ))).float().prod(1).sum() #use XNOR which is 'not+XOR'\n","\n","            loss = criterion(output_manual.float().cuda(), target)\n","            test_loss += loss.item()\n","            # print(output, loss.item())\n","\n","        correct_tol = deepcopy(correct_ocp) + deepcopy(correct_emp) #just sum\n","        accuracy_tol = 100*correct_tol/total\n","        accuracy_pd = 100*correct_ocp/total_ocp\n","        accuracy_pfa = 100*correct_emp/total_emp\n","        accuracy_cmb = 100*correct_cmb/total_cmb\n","\n","    loss = loss.item()\n","    print('test last batch',(~(output_manual^target.bool().cpu())).float().prod(1).sum() / target.size(0))\n","    print('++++++++++++++++++ Accuracy on global set: total: %d %%, combination: %d %%, PD: %d %%, PFA: %d %%, loss: %.3f' \\\n","                                            % ( accuracy_tol, accuracy_cmb, accuracy_pd, 100-accuracy_pfa, loss))\n","    return (accuracy_tol, accuracy_pd, 100-accuracy_pfa)\n","\n","\n","def adjust_learning_rate(optimizer, epoch):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n","    des = 0.1 #0.1\n","    epoch_inter = 20\n","    bs = 0.2\n","    # bs = 1\n","    lr = 0.05 * bs ** (epoch//epoch_inter)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","def train(model, epoch, class_list, train_loader, thresh_sig = 0.5):\n","    '''Use NodeDatasetMaker by default, trainloader only gives local data and only trains a local model'''\n","    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4) #decay=0 \n","    # optimizer = optim.Adam(model.parameters(), lr=0.1, weight_decay=1e-4) #\n","    # optimizer = optim.AdamW(model.parameters(), lr=0.1, weight_decay=1e-4) #\n","    adjust_learning_rate(optimizer, epoch)\n","    sys.stdout.flush()\n","\n","    for param_group in optimizer.param_groups:\n","        print('Learning Rate: %f' % param_group['lr'])\n","    model.train()\n","    thresh_logit = -1*(math.log(thresh_sig**(-1) -1))\n","    train_loss = 0\n","    criterion = nn.BCEWithLogitsLoss()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)     \n","        #print( data.size() )\n","        data, target = Variable(data*1e7), Variable(target)\n","        optimizer.zero_grad()\n","        #output = model(data, len(class_list))\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()#remains to be edited\n","    return model\n","\n","def gather_layer_MDLT(model): # Gather layers of a model to 3 categories: convlist, batchnormlist, fclist\n","    convlist = []\n","    batchnormlist= []\n","    \n","    for layer in model.features:\n","        if isinstance(layer, nn.Conv2d):\n","            convlist.append(layer)\n","        if isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.GroupNorm):\n","            batchnormlist.append(layer)\n","    fclist =model.fc # should be nn.modulelist\n","    return convlist, batchnormlist, fclist\n","\n","\n","def Merge_MDLT(net_list, net_t, idx_list, coef_list, shared_layers, class_dir): #for adjustable layers in \"256 256 256\"\n","#--------------------------------------create and zero dic_tol-----------------------------------\n","    net_tol = deepcopy(net_t)\n","    dict_tol = net_tol.state_dict()\n","\n","    for name in list( dict_tol ):  #zero dict_tol\n","        dict_tol[name].data.zero_()\n","\n","    net_tol.load_state_dict(dict_tol) #make zero net_tol\n","    convlist_n0, batchnormlist_n0, fclist_n0  = gather_layer_MDLT(net_tol)\n","#----------------------------------------Add node params to net_tol------------------------------------------\n","    for i in range( len(net_list) ):\n","        convlist_ni, batchnormlist_ni, fclist_ni  = gather_layer_MDLT(net_list[i])\n","        #---------------------------------------Merge shared layers------------------------\n","        for [m0, mi] in zip( convlist_n0[0:shared_layers], convlist_ni[0:shared_layers] ):\n","            m0.weight.data += mi.weight.data /len(class_dir)\n","            m0.bias.data += mi.bias.data /len(class_dir) \n","\n","        for [m0, mi] in zip( batchnormlist_n0[0:shared_layers], batchnormlist_ni[0:shared_layers] ):\n","            m0.weight.data += mi.weight.data /len(class_dir) # batchnormlist has weight?\n","            m0.bias.data += mi.bias.data /len(class_dir)\n","            if isinstance(m0, nn.BatchNorm2d):\n","                m0.running_mean.data += mi.running_mean.data /len(class_dir)\n","                m0.running_var.data += mi.running_var.data /len(class_dir)\n","        #-----------------------------------------Merge (add) Critical Paths 256 Convs----------------------------\n","        for [m0, mi] in zip( convlist_n0[shared_layers:], convlist_ni[shared_layers:] ):\n","            for j in range( len(idx_list[i]) ):\n","                idx = idx_list[i][j]\n","                Lx1 = mi.weight.data.size(0) // len(idx_list[i])\n","                #print(m0.weight.data.size(), mi.weight.data.size())\n","                m0.weight.data[ idx*Lx1 : (idx+1)*Lx1 ] += mi.weight.data[ j*Lx1: (j+1)*Lx1 ] /coef_list[ idx ]\n","                m0.bias.data[ idx*Lx1 : (idx+1)*Lx1 ] += mi.bias.data[ j*Lx1: (j+1)*Lx1 ] /coef_list[ idx ]     \n","        #-----------------------------------------Merge (Add) critical paths 256 BN --------------------------------------\n","        for [m0, mi] in zip( batchnormlist_n0[shared_layers:], batchnormlist_ni[shared_layers:] ):\n","            for j in range( len(idx_list[i]) ):\n","                idx = idx_list[i][j]\n","                Lx1 = mi.weight.data.size(0) // len(idx_list[i])\n","                m0.weight.data[ idx*Lx1 : (idx+1)*Lx1 ] += mi.weight.data[ j*Lx1: (j+1)*Lx1 ] /coef_list[ idx ]\n","                m0.bias.data[ idx*Lx1 : (idx+1)*Lx1 ] += mi.bias.data[ j*Lx1: (j+1)*Lx1 ] /coef_list[ idx ]\n","\n","                if isinstance(m0, nn.BatchNorm2d):\n","                    m0.running_mean.data[ idx*Lx1 : (idx+1)*Lx1 ] += mi.running_mean.data[ j*Lx1: (j+1)*Lx1 ] /coef_list[ idx ]\n","                    m0.running_var.data[ idx*Lx1 : (idx+1)*Lx1 ] += mi.running_var.data[ j*Lx1: (j+1)*Lx1 ] /coef_list[ idx ]\n","        #-----------------------------------------Merge (Add) FC-------------------------------------------------\n","        for j in range( len(idx_list[i]) ):  # here fclist_xx is nn.modulelist \n","            idx = idx_list[i][j]\n","            fclist_n0[idx].weight.data += fclist_ni[j].weight.data/ coef_list[idx]\n","            fclist_n0[idx].bias.data += fclist_ni[j].bias.data/ coef_list[idx]\n","\n","    return net_tol    \n","\n","\n","def Split_MDLT(net_list, net_tol, idx_list, coef_list, shared_layers, class_dir):\n","\n","    convlist_n0, batchnormlist_n0, fclist_n0  = gather_layer_MDLT(net_tol)\n","    # shared_layers= 1\n","    list_shared = list( range(shared_layers) )\n","    for i in range(len(net_list)):\n","        convlist_ni, batchnormlist_ni, fclist_ni  = gather_layer_MDLT(net_list[i])\n","        #---------------------------Split Shared Layers-----------------------------------------\n","        for a in list_shared: # Split layers marked in list_shared\n","            #kk = 0\n","            for [m0, mi] in zip( convlist_n0[a:a+1], convlist_ni[a:a+1] ):       \n","                #kk += 1\n","                mi.weight.data = deepcopy(m0.weight.data) # for convs in common, just update             \n","                mi.bias.data = deepcopy(m0.bias.data) # for convs in common, just update       \n","            for [m0, mi] in zip( batchnormlist_n0[a:a+1], batchnormlist_ni[a:a+1] ):\n","                mi.weight.data = deepcopy(m0.weight.data)\n","                mi.bias.data = deepcopy(m0.bias.data)\n","                if isinstance(m0, nn.BatchNorm2d):\n","                    mi.running_mean.data = deepcopy(m0.running_mean.data)\n","                    mi.running_var.data = deepcopy(m0.running_var.data)\n","        #--------------------------Split Critical paths, convs ----------------------------------\n","        for [m0, mi] in zip( convlist_n0[shared_layers:], convlist_ni[shared_layers:] ):\n","            for j in range( len( idx_list[i] ) ): # dealwith every critical pth on node\n","                idx = idx_list[i][j]\n","                Lx1 = mi.weight.data.size(0) // len(idx_list[i])\n","                mi.weight.data[ j*Lx1 : (j+1)*Lx1 ] = deepcopy(m0.weight.data[ idx*Lx1 : (idx+1)*Lx1 ])\n","                mi.bias.data[ j*Lx1 : (j+1)*Lx1 ] = deepcopy(m0.bias.data[ idx*Lx1 : (idx+1)*Lx1 ])\n","        #--------------------------Split Critical paths, BN -------------------------------------\n","        for [m0, mi] in zip( batchnormlist_n0[shared_layers:], batchnormlist_ni[shared_layers:] ):\n","            for j in range( len( idx_list[i] ) ):\n","                idx = idx_list[i][j]\n","                Lx1 = mi.weight.data.size(0) // len(idx_list[i])\n","                mi.weight.data[ j*Lx1 : (j+1)*Lx1 ] = deepcopy(m0.weight.data[ idx*Lx1 : (idx+1)*Lx1 ])\n","                mi.bias.data[ j*Lx1 : (j+1)*Lx1 ] = deepcopy(m0.bias.data[ idx*Lx1 : (idx+1)*Lx1 ])\n","\n","                if isinstance(m0, nn.BatchNorm2d):\n","                    mi.running_mean.data[ j*Lx1 : (j+1)*Lx1 ] = deepcopy(m0.running_mean.data[ idx*Lx1 : (idx+1)*Lx1 ])\n","                    mi.running_var.data[ j*Lx1 : (j+1)*Lx1 ] = deepcopy(m0.running_var.data[ idx*Lx1 : (idx+1)*Lx1 ])       \n","        #-------------------------Split FC layerrrr ---------------------------------------------\n","        for j in range( len(idx_list[i]) ): # fclist_nX is nn.modulelist\n","            idx = idx_list[i][j]\n","            fclist_ni[j].weight.data = deepcopy(fclist_n0[idx].weight.data)\n","            fclist_ni[j].bias.data = deepcopy(fclist_n0[idx].bias.data)\n","\n","    return net_list\n","\n","def Dis_analysis(class_dir, tol_list):\n","  #idx_list: list of lists, global positions of locally observable bands for each node\n","  #coef_list: coef for averaging the param for each band, how many nodes are learning each certain band\n","    idx_list = []\n","    coef_list = [0]*len( tol_list )\n","    for i in range( len(class_dir) ): #Generating the mapping btw nodes and net_tol\n","        sub_idx_list = [] \n","        for j in class_dir[i]:\n","            for k in range( len(tol_list) ):\n","                if j == tol_list[k]:\n","                    sub_idx_list.append(k)\n","                    coef_list[k] += 1 \n","                    break\n","        idx_list.append(sub_idx_list)\n","    return idx_list, coef_list\n","\n","\n","\"\"\"626/720 dataset maker, each logit corresponds to the occupation of a single channel\"\"\"\n","class TotalDatasetMaker(Dataset):\n","    \"simple version that requires the user to edit input/label format elsewhere\"\n","    def __init__(self, db, label_list, transformFunc ):\n","        \"\"\"\n","        db: a list of input signal tensors, label_list: a list of data labels, corresponding to db.\n","        \"\"\"\n","        self.datasets = db\n","        self.label_list = label_list\n","        self.transformFunc = transformFunc\n","    def __getitem__(self, i):\n","        img = self.datasets[i]\n","        img = self.transformFunc(img)\n","        class_label = self.label_list[i]\n","        return img, class_label\n","\n","    def __len__(self):\n","        return len(self.label_list)\n","    \n","    \n","\"\"\"720 dataset maker, data looks like: each global channel occupation condition ==> \n","each node only learn from 'local' received signal(full size for each node) which is an element in the list of this condition \"\"\"\n","class NodeDatasetMaker(Dataset):\n","\n","    def __init__(self, db, label_list, node, class_dir, transformFunc ):\n","        \"\"\"\n","        db: a list of input signal tensors, label_list: a list of data labels, corresponding to db.\n","        node\n","        \"\"\"\n","        self.datasets = db\n","        self.label_list = label_list\n","        self.transformFunc = transformFunc\n","        self.chn_list = class_dir[node]\n","        self.node = node\n","    def __getitem__(self, i):\n","        img = self.datasets[i][self.node]\n","        img = self.transformFunc(img)\n","        class_label = self.label_list[i][self.chn_list]\n","        return img, class_label\n","\n","    def __len__(self):\n","        return len(self.label_list)\n","\n","def setDir(filepath):\n","  # if directory not exist, create. if directory already exist, empty it.\n","  if not os.path.exists(filepath):\n","    os.makedirs(filepath)\n","  else:\n","    print('Directory already exists')\n","    shutil.rmtree(filepath, ignore_errors = True)\n","    os.mkdir(filepath)\n","       \n","print('start')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ZHgnmpyNtSz1rzY-jfCJmJtzXpie1sgl"},"id":"0-sijF0E0zHs","outputId":"ee8d70e1-21ca-4a90-f24a-4d9795249077"},"outputs":[],"source":["for snr in [8]: #: #[12, 8 ]: #[12, 8, 14, 16, 20][12, 14, 16, 4, 2 ]\n","    roc_dots = 100\n","    volum = 20\n","    SNR= -1*snr \n","    nepoch = 0\n","    DistAmp_tr = 10 # DistAmp = 10 #25\n","    DistAmp_te = 10\n","    alpha_tr = 3.71\n","    alpha_te = 3.71\n","    gain_dif= (DistAmp_te*2*3**0.5 /3)**alpha_te / (DistAmp_tr*2*3**0.5 /3)**alpha_tr #(DistAmp_te/DistAmp_tr)**3.71 # 3.71 is alpha in pathloss. gain_dif is used to normalize test data\n","    gain_dif=0.9 # 0.9\n","    stage_dir='/SNRs/'#for naming and directory\n","\n","    datadir = 'RefinedNewData/SNRs/RandMod/Data_SNR'+str(SNR)+'vol1'+'.pth' # RandMod: revision dataset, PU mod varying (testing data only)\n","    datadir_te = datadir\n","    datadir_tr = datadir\n","    Model_dir = 'Saved_Models/decouple_cnn_mod/LoadedModel/'+str(SNR)+'dBVol20' # 2023 models\n","    Model_dir = 'Saved_Models/decouple_cnn_mod/SNRs/RandMod_Retrain/-4dBVol20_OLD'#-4dB retrained CEL\n","\n","    data_dict_tr = torch.load(datadir_tr)\n","    data_dict_te = torch.load(datadir_te)\n","    data_dict_tr.keys()\n","\n","    db = data_dict_tr['training data list']\n","    label_list = data_dict_tr['training label list']\n","\n","    db_te = data_dict_te['testing data list']\n","    label_list_te = data_dict_te['testing label list']\n","\n","    # Create models ==================================================================================================\n","    per_class_filter = 8\n","    #fms = 16 #size of final conv feature map\n","    shared_layers = 1\n","    class_dir=[[0, 3, 4, 19], [0, 1, 10, 4, 19], [1, 10, 4, 19, 5, 13], [1, 10, 2, 11, 14, 5, 13], [2, 11, 14, 5, 13, 6, 15, 17], [3, 4, 19, 7, 12, 18], [4, 19, 7, 12, 18, 8, 16], [4, 19, 5, 13, 8, 16], [5, 13, 8, 16, 9], [5, 13, 6, 15, 17, 9]]\n","\n","    tol_list = []\n","    for classi in class_dir:\n","        tol_list += classi\n","    tol_list = list( set(tol_list) )\n","    tol_list.sort()\n","\n","    print(class_dir)\n","    print(tol_list)\n","\n","    # get coef \n","    idx_list, coef_list = Dis_analysis(class_dir, tol_list)\n","    print(idx_list, coef_list)\n","\n","    #Parepare nets\n","    net_list = []\n","    net_tol = create_net(tol_list, per_class_filter, True) \n","    Acc_tol=[]\n","    Acc_list = []\n","\n","    for i in range(len(class_dir)):\n","        net_list.append(create_net(class_dir[i], per_class_filter, True))\n","        Acc_list.append([])\n","\n","    db_tr_list = []\n","    for idx in range(len(class_dir)):# train datasets are more complex\n","        db_tr_list.append(NodeDatasetMaker( db, label_list, idx, class_dir, transforms.Compose([ ]) ))\n","    db_te_1 = TotalDatasetMaker( db_te, label_list_te, transforms.Compose([ ]) )\n","\n","    train_loader_list = []\n","    for idx in range(len(class_dir)):# trainloaders are more complex\n","        train_loader_list.append(DataLoader(db_tr_list[idx], batch_size=50, shuffle=True, num_workers=0, pin_memory=True))\n","    # tol_train_loader = DataLoader(tol_trainsets, batch_size=100, shuffle=False, num_workers=0, pin_memory=True)\n","    tol_test_loader = DataLoader(db_te_1, batch_size=1024, shuffle=False, num_workers=4, pin_memory=True)\n","\n","    now=datetime.datetime.now(TMZ) #time watermark\n","    time_watermark = now.strftime('%y%m%d_%H_%M')\n","    print('model watermark',time_watermark)\n","    address_model = 'Saved_Models/'+type(net_list[0]).__name__+stage_dir+str(SNR)+'dBVol'+str(volum)+'_'+time_watermark+'/' #root dir for saved models\n","    #child dir nbamed by time_watermark\n","    setDir(address_model+'checkpoint/') # if dir not exist, create. if dir already exist, empty it.\n","    setDir(address_model+'bestmodel/')\n","    # name1 = type(v).__name__ +'_SNR'+str(SNR)+'vol'+str(volum)+'.pth'\n","    print('Models saved to dir:\\n', address_model)\n","    name0 = type(net_list[0]).__name__ +'_SNR'+str(SNR)+'vol'+str(volum) # common part of DNN node names\n","\n","    txt=open(address_model+'Datasetdir.txt',\"w\").write(datadir_tr) #save dataset dir (dataset version)\n","\n","    # Train models ==================================================================================================\n","    aggre_inter = 1\n","    net_tol  = Merge_MDLT(net_list, net_tol, idx_list, coef_list, shared_layers, class_dir) #global varaible\n","    net_list = Split_MDLT(net_list, net_tol, idx_list, coef_list,  shared_layers, class_dir)\n","\n","    Acc_PD = []\n","    Acc_PFA = []\n","    Acc_cmb = []\n","    Acc_tol = []\n","    Acc_bfMerge = [] #acc before merging nets\n","    plt.title(\"Global Model ACC of the proposed method\")\n","    best_acc = 0\n","    best_acc_bfMerge = 0\n","    result = testnetsVote(net_list, class_dir, tol_test_loader, coef_list, gain_dif, thresh_sig = 0.5)\n","    Acc_tol.append( result[0].item()  )\n","    Acc_PD.append( result[1].item())\n","    Acc_PFA.append( 100-result[2].item())\n","    Acc_bfMerge.append(1*Acc_tol[0])\n","    for epoch in range(nepoch):\n","        # Train & save dicts of n1 n2\n","        time_start = time.time()\n","        print('epoch:',epoch)\n","        for i in range(len(net_list)):\n","            net_list[i] = train(net_list[i], epoch, class_dir[i], train_loader_list[i])     \n","\n","        print(\"Aggregation every %d epoch, current_interval %d\" %(aggre_inter, (epoch % aggre_inter)))\n","        # result = testnets( net_list, class_dir, tol_test_loader, thresh_sig = 0.5 )\n","        result = testnetsVote(net_list, class_dir, tol_test_loader, coef_list, gain_dif, thresh_sig = 0.5)\n","        Acc_bfMerge.append( result[0].item())\n","        Acc_PD.append( result[1].item())\n","        Acc_PFA.append( 100-result[2].item()) # outputs PFA\n","\n","        local_state = { # save locally trained models\n","        'net_dict_list': [net.state_dict() for net in net_list],\n","        'Acc': Acc_bfMerge[-1],\n","        'epoch': epoch,\n","        }\n","        name1 = address_model+'checkpoint/'+name0+ 'local_nodes'+'.pth'\n","        torch.save(local_state, name1)\n","\n","        if Acc_bfMerge[-1] > best_acc_bfMerge: #save locally trained best model\n","            name1 = address_model+'bestmodel/'+name0+ 'local_nodes'+'.pth'\n","            torch.save(local_state, name1)\n","            best_acc_bfMerge = 1*Acc_bfMerge[-1]\n","\n","        if ((epoch+1) % aggre_inter == 0):\n","            net_tol  = Merge_MDLT(net_list, net_tol, idx_list, coef_list, shared_layers, class_dir) #global varaible\n","            print('@@@===@@@===@@@===@@@===@@@===@@@===@@@===@@@==== Merged nets')\n","            #Split nets, test;\n","            net_list = Split_MDLT(net_list, net_tol, idx_list, coef_list,  shared_layers, class_dir)\n","            print('=================================================== Splitted nets')\n","            Acc_tol.append( testnetsVote(net_list, class_dir, tol_test_loader, coef_list, gain_dif, thresh_sig = 0.5)[0].item() )\n","\n","            Merged_state = { # local models and global model after merge\n","            'net_dict_list': [net.state_dict() for net in net_list],\n","            'net_tol': net_tol.state_dict(),\n","            'acc': Acc_tol[-1],\n","            'epoch': epoch, \n","            }\n","            name1 = address_model+'checkpoint/'+name0+ 'merged_nodes'+'.pth'\n","            torch.save(Merged_state, name1)\n","\n","            if Acc_tol[-1] > best_acc:\n","                name1 = address_model+'bestmodel/'+name0+ 'merged_nodes'+'.pth'\n","                torch.save(Merged_state, name1)\n","                Best_acc = 1*Acc_tol[-1]\n","            \n","        plt.figure(1,figsize=(5, 4), dpi=80)\n","        l1, = plt.plot( Acc_tol, color='blue', label='Avg Acc/band')\n","        l2, = plt.plot( Acc_PFA, color='red', label='Acc 4 empty')\n","        l3, = plt.plot( Acc_PD, color='black', label='Acc 4 busy')\n","        # l4, = plt.plot( Acc_cmb, color='green', label='Acc 4 Combine')\n","        plt.title('SNR='+str(SNR)+'dB,'+ type(net_list[0]).__name__+ ' Model ACC reaches %.3f %%' %  (max(Acc_bfMerge))  )\n","        plt.legend(loc='lower right')\n","        plt.show()\n","\n","        plt.figure(2,figsize=(5, 4), dpi=80)\n","        l1, = plt.plot( Acc_tol, color='blue',label='Acc after merge')\n","        l2, = plt.plot( Acc_bfMerge, color='red', label='Acc before merge')\n","        plt.legend(loc='lower right')\n","        plt.title('SNR='+str(SNR)+'dB,'+ type(net_list[0]).__name__+ ' Model ACC reaches %.3f %%(after merge) and %.3f %%(before merge)' %  (max(Acc_tol), max(Acc_bfMerge)) )\n","        plt.show()        \n","\n","        time_end=time.time()\n","        print('1 epoch time cost:',time_end-time_start,'s')\n","\n","    Merged_state = torch.load(Model_dir+'/bestmodel/'+name0+ 'merged_nodes'+'.pth')\n","    for i in range(len(net_list)):\n","      net_list[i].load_state_dict(Merged_state['net_dict_list'][i])\n","    # local_state['net_dict_list']\n","    net_tol.load_state_dict( Merged_state['net_tol'])\n","\n","    # ROC  ==================================================================================================\n","    '''ROC module of current standalone model, saved in pd2 and pfa2'''\n","    pd2= []\n","    pfa2 = []\n","\n","    for thresh_val in [ (i+.99999)/200 for i in range(200)]:  \n","        print('threshold:', thresh_val)\n","        CNNoutput = testnetsVote(net_list, class_dir, tol_test_loader, coef_list, gain_dif, thresh_sig = thresh_val)\n","        pd2.append(CNNoutput[1].to(torch.device('cpu')).item())\n","        pfa2.append(CNNoutput[2].to(torch.device('cpu')).item())\n","\n","    plt.title(\"ROC of \" +type(net_list[0]).__name__+ \" method in SNR=\"+str(SNR)+\"dB\")\n","    l2, = plt.plot(pfa2, pd2, color='green', label='Transformer')\n","    plt.legend(loc='lower right')\n","    plt.show()\n","\n","    dfroc = pd.DataFrame() # save statics to excel\n","    # df1['acc_old'] = xx\n","    \n","    dfroc['PFA'] = pfa2\n","    dfroc['PD'] = pd2\n","    with pd.ExcelWriter(address_model + \"ROC_SNR\"+str(SNR)+\".xlsx\", mode='w') as writer:  #mode was 'a'\n","      dfroc.to_excel(writer, sheet_name=type(net_list[0]).__name__)\n","    print('ROC in Excel saved to:', address_model + \"ROC_SNR\"+str(SNR)+\".xlsx\")\n","\n","    ROC_dict = {\n","        'pd':pd2,\n","        'pfa':pfa2,\n","    }\n","    torch.save(ROC_dict, address_model+type(net_list[0]).__name__+'ROC.pth')\n","    print('ROC in Lists saved to:', address_model+type(net_list[0]).__name__+'ROC.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZRkjN_XEO2t"},"outputs":[],"source":["from torchsummary import summary \n","# v = AlexNet1D(num_classes = 10).to(device)\n","summary(net_list[0], (1,64,20))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","\n","# To stop the kernel using sys.exit()\n","sys.exit()\n","\n","# To stop the kernel using KeyboardInterrupt\n","raise KeyboardInterrupt"]}],"metadata":{"accelerator":"GPU","celltoolbar":"Tags","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
