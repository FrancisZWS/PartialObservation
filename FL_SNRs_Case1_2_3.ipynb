{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7721,"status":"ok","timestamp":1682543816684,"user":{"displayName":"Weishan Zhang","userId":"02899996824368835811"},"user_tz":240},"id":"WQhOO-fm0zHk","outputId":"a55cc382-e3ef-4d24-b05f-e602e5a822a4","scrolled":true},"outputs":[],"source":["'''Office server version Jan 2024\n","FL for multiple SNRs, updated Apr2023, based on CEL, in progress\n","class_dir change, test change (use coef_list, also update to stdaln and cel, )\n","make MergeFL, SplitFL for merge-split\n","\n","CEL vs Energy detection, 2022 Mar 14 Trying on Colab. \n","It seems that processing small batch is slow, 7s per epoch per user\n","Big batch can accelerate to less than 2s per epoch per user\n","Copied on 2023Feb3'''\n","import torch\n","import os\n","import math\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.utils.data.distributed as TUDdistributed\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from torch.autograd import Variable\n","import sys\n","from copy import deepcopy\n","import random\n","import collections\n","#from MobileNetV1_CriticalPath import Net\n","import matplotlib.pyplot as plt\n","import pandas as pd \n","# from sklearn.externals import joblib\n","from cnn_models import standalone_cnn\n","from cnn_models import decouple_cnn\n","from cnn_models import decouple_cnn_mod\n","\n","from pytz import timezone\n","TMZ = timezone('EST')\n","import datetime\n","import shutil\n","import time\n","\n","device = torch.device(\"cuda\")\n","use_cuda = True\n","criterion = nn.CrossEntropyLoss()\n","random.seed(0) \n","\n","def create_net(list, per_class_filter, ty_chs):\n","    number_class = len(list) # list of locally observable channels\n","    d_w = number_class*per_class_filter\n","    cfg = [40, d_w, d_w, d_w ]\n","    print('corresponding cfg channel list:',cfg)\n","    net = standalone_cnn( nch=number_class, cfg=cfg, ty_chs=ty_chs)\n","    net.to(device)\n","    return net\n","\n","def testnetsVote(model_list, class_dir, test_loader, coef_list, gain_dif, thresh_sig = 0.5):                             \n","    '''default for FL, Tests all nodes together, apply majority vote for each band \n","    '''\n","    total = 0 #sum of occupation and emptiness\n","    test_loss = 0 \n","    total_ocp = 0 #occupied bands\n","    total_emp = 0 #empty bands\n","    correct = 0\n","    total_cmb = 0  #occupation combination\n","    correct_ocp = 0\n","    correct_emp = 0\n","    correct_cmb = 0\n","    thresh_logit = -1*(math.log(thresh_sig**(-1) -1))\n","    # thresh_logit = thresh_sig\n","    \n","    with torch.no_grad():\n","        criterion = nn.BCEWithLogitsLoss()\n","        for batchidx, (data, target) in enumerate(test_loader):\n","            target =  target.cuda()\n","            #print(target.size())\n","            target = Variable(target)\n","            target = Variable(target)\n","#             print('target size is:',target.size())\n","            output_manual = torch.zeros(target.shape) # manual global decision, float version\n","            total_batch = float(torch.tensor(target.size()).prod()) #total num of channels in this batch\n","            total += total_batch\n","            total_cmb += float(target.size(0))\n","            total_ocp += float(target.sum())\n","            total_emp += float(total_batch - target.sum()) \n","            if (1-target).sum() != (torch.tensor(target.size()).prod() - target.sum()) : #Testing tensor dim =========\n","                print('show difference:',(1-target).sum(), total - target.sum())\n","                print('total ocp/emp calculation wrong')\n","            '''Above calculate the total number of channels detected and the ground truth number of occupation/emptiness'''\n","            coef = torch.tensor(coef_list)\n","            for idx in range(len(class_dir)): # appply fusion strategy on local_node detections\n","                model_list[idx].eval()\n","                # model_list[idx](data[idx])\n","                datain = Variable(gain_dif*1e7*data[idx]).cuda() #1e7 to boost gradient\n","                localout = (model_list[idx](datain).cpu()>thresh_logit).float()\n","                output_manual[:,class_dir[idx]]=output_manual[:,class_dir[idx]].add_( localout[:,class_dir[idx]] ) #add local detection result\n","                # output_manual[:,class_dir[idx]] = output_manual[:,class_dir[idx]]|(model_list[idx](datain).cpu()>thresh_logit)\n","            output_manual = output_manual.div_( coef ) > 0.49 #get decision fusion: sum observable output and normalize \n","\n","            correct_ocp += (output_manual*target.cpu()).sum()\n","            correct_emp += ((~output_manual)*(1-target.cpu())).sum()\n","            correct_cmb += (~(output_manual^( target.bool().cpu() ))).float().prod(1).sum() #use XNOR which is 'not+XOR'\n","\n","            loss = criterion(output_manual.float().cuda(), target)\n","            test_loss += loss.item()\n","            # print(output, loss.item())\n","\n","        correct_tol = deepcopy(correct_ocp) + deepcopy(correct_emp) #just sum\n","        accuracy_tol = 100*correct_tol/total\n","        accuracy_pd = 100*correct_ocp/total_ocp\n","        accuracy_pfa = 100*correct_emp/total_emp\n","        accuracy_cmb = 100*correct_cmb/total_cmb\n","\n","    loss = loss.item()\n","    print('test last batch',(~(output_manual^target.bool().cpu())).float().prod(1).sum() / target.size(0))\n","    print('++++++++++++++++++ Accuracy on global set: total: %d %%, combination: %d %%, PD: %d %%, PFA: %d %%, loss: %.3f' \\\n","                                            % ( accuracy_tol, accuracy_cmb, accuracy_pd, 100-accuracy_pfa, loss))\n","    return (accuracy_tol, accuracy_pd, 100-accuracy_pfa)\n","\n","\n","def testnets4FL(model_list, class_list, test_loader, thresh_sig = 0.5):                             \n","    '''Tests all nodes together, use TotalDatasetMaker, for FL models but partial_observ\n","    strategy: any node detects means occupation, only use observ_bands'''\n","    total = 0 #sum of occupation and emptiness\n","    test_loss = 0 \n","    total_ocp = 0 #occupied bands\n","    total_emp = 0 #empty bands\n","    correct = 0\n","    total_cmb = 0  #occupation combination\n","    correct_ocp = 0\n","    correct_emp = 0\n","    correct_cmb = 0\n","    thresh_logit = -1*(math.log(thresh_sig**(-1) -1))\n","    # thresh_logit = thresh_sig\n","    \n","    with torch.no_grad():\n","        criterion = nn.BCEWithLogitsLoss()\n","        for batchidx, (data, target) in enumerate(test_loader):\n","            target =  target.cuda()\n","            #print(target.size())\n","            target = Variable(target)\n","            target = Variable(target)\n","#             print('target size is:',target.size())\n","            output_manual = torch.zeros(target.shape).bool() # manual global decision\n","            total_batch = float(torch.tensor(target.size()).prod()) #total num of channels in this batch\n","            total += total_batch\n","            total_cmb += float(target.size(0))\n","            total_ocp += float(target.sum())\n","            total_emp += float(total_batch - target.sum()) \n","            if (1-target).sum() != (torch.tensor(target.size()).prod() - target.sum()) : #Testing tensor dim =========\n","                print('show difference:',(1-target).sum(), total - target.sum())\n","                print('total ocp/emp calculation wrong')\n","            '''Above calculate the total number of channels detected and the ground truth number of occupation/emptiness'''\n","\n","            for idx in range(len(class_dir)): # appply fusion strategy on local detections\n","                model_list[idx].eval()\n","                # model_list[idx](data[idx])\n","                data[idx]= Variable(1e7*data[idx])\n","                output_manual[:,class_list[idx]] = output_manual[:,class_list[idx]]|(model_list[idx](data[idx].cuda())[:,class_list[idx]].cpu()>thresh_logit)\n","                # strategy: any node detects means occupation, only use observ_bands\n","            correct_ocp += (output_manual*target.cpu()).sum()\n","            correct_emp += ((~output_manual)*(1-target.cpu())).sum()\n","            correct_cmb += (~(output_manual^( target.bool().cpu() ))).float().prod(1).sum() #use XNOR which is 'not+XOR'\n","\n","            loss = criterion(output_manual.float().cuda(), target)\n","            test_loss += loss.item()\n","            # print(output, loss.item())\n","\n","        correct_tol = deepcopy(correct_ocp) + deepcopy(correct_emp) #just sum\n","        accuracy_tol = 100*correct_tol/total\n","        accuracy_pd = 100*correct_ocp/total_ocp\n","        accuracy_pfa = 100*correct_emp/total_emp\n","        accuracy_cmb = 100*correct_cmb/total_cmb\n","\n","    loss = loss.item()\n","    print('test last batch',(~(output_manual^target.bool().cpu())).float().prod(1).sum() / target.size(0))\n","    print('++++++++++++++++++ Accuracy on global set: total: %d %%, combination: %d %%, PD: %d %%, PFA: %d %%, loss: %.3f' \\\n","                                            % ( accuracy_tol, accuracy_cmb, accuracy_pd, 100-accuracy_pfa, loss))\n","    return (accuracy_tol, accuracy_pd, 100-accuracy_pfa)\n","\n","\n","\n","def adjust_learning_rate(optimizer, epoch):\n","    '''Sets the learning rate to the initial LR decayed by 10 every 30 epochs'''\n","    epoch_inter = 20\n","    bs = 0.2\n","    # bs = 1\n","    lr = 0.05 * bs ** (epoch//epoch_inter)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","def train(model, epoch, class_list, train_loader, thresh_sig = 0.5):\n","    '''Use NodeDatasetMaker by default, trainloader only gives local data and only trains a local model'''\n","    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=3e-4) #decay=0 \n","    # optimizer = optim.Adam(model.parameters(), lr=0.1, weight_decay=1e-4) #\n","    # optimizer = optim.AdamW(model.parameters(), lr=0.1, weight_decay=1e-4) #\n","    adjust_learning_rate(optimizer, epoch)\n","#     print(\"\\nLocal Epoch\", epoch)\n","    sys.stdout.flush()\n","\n","    for param_group in optimizer.param_groups:\n","        print('Learning Rate: %f' % param_group['lr'])\n","    # sys.stdout.flush()\n","    model.train()\n","    thresh_logit = -1*(math.log(thresh_sig**(-1) -1))\n","#     print('threshold on output logits', thresh_logit)\n","    train_loss = 0\n","    criterion = nn.BCEWithLogitsLoss()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)     \n","        #print( data.size() )\n","        data, target = Variable(data*1e7), Variable(target)\n","        optimizer.zero_grad()\n","        #output = model(data, len(class_list))\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()#remains to be edited\n","    return model\n","\n","def gather_layer_FL(model): # Gather layers of a model to 3 categories: convlist, batchnormlist, fclist\n","    convlist = []\n","    batchnormlist= []\n","    \n","    for layer in model.features:\n","        if isinstance(layer, nn.Conv2d):\n","            convlist.append(layer)\n","        if isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.GroupNorm):\n","            batchnormlist.append(layer)   \n","    nnfc = model.fc # should be nn.modulelist\n","    return convlist, batchnormlist, nnfc\n","\n","\n","def Merge_FL(net_list, net_tol, idx_list, class_dir): \n","    '''Fed-Avg Merge, dealing with normal CNN'''\n","#--------------------------------------create and zero dic_tol-----------------------------------\n","    net_tol = deepcopy(net_tol)\n","    dict_tol = net_tol.state_dict()\n","    for name in list( dict_tol ):  #zero dict_tol\n","        dict_tol[name].data.zero_()\n","\n","    net_tol.load_state_dict(dict_tol) #make zero net_tol\n","    convlist_n0, batchnormlist_n0, fc0  = gather_layer_FL(net_tol)\n","#----------------------------------------Add node params to net_tol------------------------------------------\n","    for i in range( len(net_list) ):\n","        convlist_ni, batchnormlist_ni, fci = gather_layer_FL(net_list[i])\n","        #---------------------------------------Merge shared layers------------------------\n","        for [m0, mi] in zip( convlist_n0, convlist_ni ):\n","            m0.weight.data += mi.weight.data /len(class_dir)\n","            m0.bias.data += mi.bias.data /len(class_dir) \n","        for [m0, mi] in zip( batchnormlist_n0, batchnormlist_ni ):\n","            m0.weight.data += mi.weight.data /len(class_dir) # batchnormlist has weight?\n","            m0.bias.data += mi.bias.data /len(class_dir)\n","            if isinstance(m0, nn.BatchNorm2d):\n","                m0.running_mean.data += mi.running_mean.data /len(class_dir)\n","                m0.running_var.data += mi.running_var.data /len(class_dir)\n","        #-----------------------------------------Merge (Add) FC-------------------------------------------------\n","        fc0.weight.data += fci.weight.data/ len(class_dir)\n","        fc0.bias.data += fci.bias.data/ len(class_dir)\n","\n","    return net_tol    \n","\n","\n","def Split_FL(net_list, net_tol, idx_list, class_dir):\n","\n","    convlist_n0, batchnormlist_n0, fclist_n0  = gather_layer_FL(net_tol) # actaully fclist is not list\n","    list_shared = list( range(shared_layers) )\n","    for i in range(len(net_list)):\n","        convlist_ni, batchnormlist_ni, fclist_ni = gather_layer_FL(net_list[i])\n","        #---------------------------Split Shared Layers-----------------------------------------\n","        for [m0, mi] in zip( convlist_n0, convlist_ni):       \n","            #kk += 1\n","            mi.weight.data = deepcopy(m0.weight.data) # for convs in common, just update             \n","            mi.bias.data = deepcopy(m0.bias.data) # for convs in common, just update       \n","        #print('@@@@@=====@@@@@           check itr',k)    \n","        for [m0, mi] in zip( batchnormlist_n0, batchnormlist_ni):\n","            mi.weight.data = deepcopy(m0.weight.data)\n","            mi.bias.data = deepcopy(m0.bias.data)\n","            if isinstance(m0, nn.BatchNorm2d):\n","                mi.running_mean.data = deepcopy(m0.running_mean.data)\n","                mi.running_var.data = deepcopy(m0.running_var.data)      \n","    #-------------------------Split FC layerrrr ---------------------------------------------\n","        fclist_ni.weight.data = deepcopy(fclist_n0.weight.data)\n","        fclist_ni.bias.data = deepcopy(fclist_n0.bias.data)\n","\n","    return net_list\n","\n","def Dis_analysis(class_dir, tol_list):\n","  #idx_list: list of lists, global positions of locally observable bands for each node\n","  #coef_list: coef for averaging the param for each band, how many nodes are learning each certain band\n","    idx_list = []\n","    coef_list = [0]*len( tol_list )\n","    for i in range( len(class_dir) ): #Generating the mapping btw nodes and net_tol\n","        sub_idx_list = [] \n","        for j in class_dir[i]:\n","            for k in range( len(tol_list) ):\n","                if j == tol_list[k]:\n","                    sub_idx_list.append(k)\n","                    coef_list[k] += 1 \n","                    break\n","        idx_list.append(sub_idx_list)\n","    return idx_list, coef_list\n","\n","\n","\"\"\"626/720 dataset maker, each logit corresponds to the occupation of a single channel\"\"\"\n","class TotalDatasetMaker(Dataset):\n","    \"simple version that requires the user to edit input/label format elsewhere\"\n","    def __init__(self, db, label_list, transformFunc ):\n","        \"\"\"\n","        db: a list of input signal tensors, label_list: a list of data labels, corresponding to db.\n","        \"\"\"\n","        self.datasets = db\n","        self.label_list = label_list\n","        self.transformFunc = transformFunc\n","    def __getitem__(self, i):\n","        img = self.datasets[i]\n","        img = self.transformFunc(img)\n","        class_label = self.label_list[i]\n","        return img, class_label\n","\n","    def __len__(self):\n","        return len(self.label_list)\n","    \n","    \n","\"\"\"720 dataset maker, data looks like: each global channel occupation condition ==> \n","each node only learn from 'local' received signal(full size for each node) which is an element in the list of this condition \"\"\"\n","class NodeDatasetMakerFL(Dataset):\n","    '''Node dataset maker, FedAvg version'''\n","    def __init__(self, db, label_list, node, class_dir, transformFunc ):\n","        \"\"\"\n","        db: a list of input signal tensors, label_list: a list of data labels, corresponding to db.\n","        node\n","        \"\"\"\n","        self.datasets = db\n","        self.label_list = label_list\n","        self.transformFunc = transformFunc\n","        self.chn_list = class_dir[node]\n","        self.node = node\n","    def __getitem__(self, i):\n","        img = self.datasets[i][self.node]\n","        img = self.transformFunc(img)\n","        class_label = self.label_list[i] # For FL, only change here\n","        return img, class_label\n","\n","    def __len__(self):\n","        return len(self.label_list)\n","\n","def setDir(filepath):\n","  # if directory not exist, create. if directory already exist, empty it.\n","  if not os.path.exists(filepath):\n","    os.makedirs(filepath)\n","  else:\n","    print('Directory already exists')\n","    shutil.rmtree(filepath, ignore_errors = True)\n","    os.mkdir(filepath)\n","\n","        \n","print('start')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"0-sijF0E0zHs","outputId":"086e11c9-7581-4966-fd93-32dc4dbd3fee"},"outputs":[],"source":["for snr in [12]: #[12, 8 ]: #[12, 8, 14, 16, 20] [4, 6, 8, 10, 12, 14, 16]\n","\n","    #Load data ===================#===================#===================#===================#===================#===================\n","    volum = 20\n","    batchsz=64\n","    SNR= -1*snr \n","    nepoch = 50\n","    roc_dots = 100\n","    DistAmp_tr = 10 # DistAmp = 10 #25\n","    DistAmp_te = 10\n","    alpha_tr = 3.71\n","    alpha_te = 3.71#5\n","    gain_dif= (DistAmp_te*2*3**0.5 /3)**alpha_te / (DistAmp_tr*2*3**0.5 /3)**alpha_tr    \n","    \n","\n","    datadir_tr = 'RefinedNewData/SNRs/'+str(DistAmp_tr)+'m/Data_SNR'+str(SNR)+'vol'+str(volum)+'.pth'\n","    datadir_te = 'RefinedNewData/SNRs/RandMod/Data_SNR'+str(SNR)+'vol1'+'.pth' # RandMod: revision dataset, PU mod varying (testing data only)\n","    datadir_te = 'RefinedNewData/SNRs/RandMod/Tr20Te5/Data_SNR'+str(SNR)+'vol'+str(volum)+'.pth' # RandMod: revision dataset, PU mod varying (testing data only)\n","    datadir_tr = datadir_te\n","\n","    now=datetime.datetime.now(TMZ) #time watermark\n","    time_watermark = now.strftime('%y%m%d_%H_%M')\n","    print('model watermark',time_watermark)\n","    #child dir nbamed by time_watermark\n","    \n","    stage_dir='/SNRs/RandModTrTe/'#for naming and directory\n","    '''address_model: Where to save model:'''\n","    address_model = 'Saved_Models/'+'FL'+stage_dir+str(SNR)+'dB'+str(volum)+'_'+time_watermark+'/' #root dir for saved models\n","\n","    data_dict_tr = torch.load(datadir_tr)\n","    data_dict_te = torch.load(datadir_te)\n","    data_dict_tr.keys()\n","\n","    db = data_dict_tr['training data list']\n","    label_list = data_dict_tr['training label list']\n","\n","    db_te = data_dict_te['testing data list']\n","    label_list_te = data_dict_te['testing label list']\n","\n","    #create models ===================#===================#===================#===================#===================#===================\n","    per_class_filter = 8\n","    shared_layers = 100 # for FL, larger than conv layer, \n","\n","    class_dir_real=[[0, 3, 4, 19], [0, 1, 10, 4, 19], [1, 10, 4, 19, 5, 13], [1, 10, 2, 11, 14, 5, 13], [2, 11, 14, 5, 13, 6, 15, 17], [3, 4, 19, 7, 12, 18], [4, 19, 7, 12, 18, 8, 16], [4, 19, 5, 13, 8, 16], [5, 13, 8, 16, 9], [5, 13, 6, 15, 17, 9]]\n","    class_dir = [ list( range(20) ) for i in range(10)] # every local is Full_Observa\n","    tol_list = []\n","    for classi in class_dir:\n","        tol_list += classi\n","    tol_list = list( set(tol_list) )\n","    tol_list.sort()\n","\n","    print(class_dir)\n","    print(tol_list)\n","\n","    # get coef \n","    idx_list, coef_list = Dis_analysis(class_dir_real, tol_list)\n","    print(idx_list, coef_list)\n","\n","    #Parepare nets\n","    net_list = []\n","    net_tol = create_net(tol_list, per_class_filter, True) \n","    Acc_tol=[]\n","    Acc_list = []\n","\n","    for i in range(len(class_dir)):\n","        net_list.append(create_net(class_dir[i], per_class_filter, True))\n","        Acc_list.append([])\n","\n","    db_tr_list = []\n","    for idx in range(len(class_dir)):# train datasets are more complex\n","        db_tr_list.append(NodeDatasetMakerFL( db, label_list, idx, class_dir, transforms.Compose([ ]) ))\n","    db_te_1 = TotalDatasetMaker( db_te, label_list_te, transforms.Compose([ ]) )\n","\n","    train_loader_list = []\n","    for idx in range(len(class_dir)):# trainloaders are more complex\n","        train_loader_list.append(DataLoader(db_tr_list[idx], batch_size=batchsz, shuffle=True, num_workers=8, pin_memory=True))\n","    # tol_train_loader = DataLoader(tol_trainsets, batch_size=100, shuffle=False, num_workers=0, pin_memory=True)\n","    tol_test_loader = DataLoader(db_te_1, batch_size=1024, shuffle=False, num_workers=4, pin_memory=True)\n","\n","\n","    setDir(address_model+'checkpoint/') # if dir not exist, create. if dir already exist, empty it.\n","    setDir(address_model+'bestmodel/')\n","    # name1 = type(v).__name__ +'_SNR'+str(SNR)+'vol'+str(volum)+'.pth'\n","    print('Models saved to dir:\\n', address_model)\n","    name0 = 'FL' +'_SNR'+str(SNR)+'vol'+str(volum) # common part of DNN node names\n","\n","    txt=open(address_model+'Datasetdir.txt',\"w\").write(datadir_tr) #save dataset dir (dataset version)\n","\n","    #training===================#===================#===================#===================#===================#===================\n","    aggre_inter = 1\n","\n","    net_tol  = Merge_FL(net_list, net_tol, idx_list, class_dir) #global varaible\n","    net_list = Split_FL(net_list, net_tol, idx_list, class_dir)\n","\n","    Acc_PD = []\n","    Acc_PFA = []\n","    Acc_cmb = []\n","    Acc_tol = []\n","    Acc_bfMerge = [] #acc before merging nets\n","    plt.title(\"Global Model ACC of the proposed method\")\n","    best_acc = 0 #acc after merge\n","    best_acc_bfMerge = 0\n","    result = testnetsVote( net_list, class_dir, tol_test_loader, coef_list, gain_dif, thresh_sig = 0.5 )\n","    Acc_tol.append( result[0].item()  )\n","    Acc_PD.append( result[1].item())\n","    Acc_PFA.append( 100-result[2].item())\n","    Acc_bfMerge.append(1*Acc_tol[0])\n","    for epoch in range(nepoch):\n","        time_start=time.time()\n","        # Train & save dicts of n1 n2\n","        print('epoch:',epoch)\n","        for i in range(len(net_list)):\n","            net_list[i] = train(net_list[i], epoch, class_dir[i], train_loader_list[i])     \n","        print(\"Aggregation every %d epoch, current_interval %d\" %(aggre_inter, (epoch % aggre_inter)))\n","        result = testnetsVote( net_list, class_dir_real, tol_test_loader, coef_list, gain_dif, thresh_sig = 0.5 )\n","\n","        # Acc_bfMerge.append( testnets( net_list, class_dir, tol_test_loader )[0].item()  )\n","        Acc_bfMerge.append( result[0].item())\n","        Acc_PD.append( result[1].item())\n","        Acc_PFA.append( 100-result[2].item()) # outputs PFA\n","\n","        local_state = { # save locally trained models\n","        'net_dict_list': [net.state_dict() for net in net_list],\n","        'Acc': Acc_bfMerge[-1],\n","        'epoch': epoch,\n","        }\n","        name1 = address_model+'checkpoint/'+name0+ 'local_nodes'+'.pth'\n","        torch.save(local_state, name1)\n","\n","        if Acc_bfMerge[-1] > best_acc_bfMerge: #save locally trained best model\n","            name1 = address_model+'bestmodel/'+name0+ 'local_nodes'+'.pth'\n","            torch.save(local_state, name1)\n","            best_acc_bfMerge = 1*Acc_bfMerge[-1]\n","\n","        if ((epoch+1) % aggre_inter == 0):\n","            net_tol  = Merge_FL(net_list, net_tol, idx_list, class_dir) #global varaible\n","\n","            print('@@@===@@@===@@@===@@@===@@@===@@@===@@@===@@@==== Merged nets')\n","            #Split nets, test;\n","            net_list = Split_FL(net_list, net_tol, idx_list, class_dir)\n","            print('=================================================== Splitted nets')\n","            Acc_tol.append( testnetsVote( net_list, class_dir_real, tol_test_loader, coef_list, gain_dif, thresh_sig = 0.5 )[0].item()  )\n","            Merged_state = { # local models and global model after merge\n","            'net_dict_list': [net.state_dict() for net in net_list],\n","            'net_tol': net_tol.state_dict(),\n","            'acc': Acc_tol[-1],\n","            'epoch': epoch, \n","            }\n","            name1 = address_model+'checkpoint/'+name0+ 'merged_nodes'+'.pth'\n","            torch.save(Merged_state, name1)\n","\n","            if Acc_tol[-1] > best_acc:\n","                name1 = address_model+'bestmodel/'+name0+ 'merged_nodes'+'.pth'\n","                torch.save(Merged_state, name1)\n","                Best_acc = 1*Acc_tol[-1]         \n","\n","        plt.figure(1,figsize=(5, 4), dpi=80)\n","        l1, = plt.plot( Acc_tol, color='blue', label='Avg Acc/band')\n","        l2, = plt.plot( Acc_PFA, color='red', label='Acc 4 empty')\n","        l3, = plt.plot( Acc_PD, color='black', label='Acc 4 busy')\n","        # l4, = plt.plot( Acc_cmb, color='green', label='Acc 4 Combine')\n","        plt.title('SNR='+str(SNR)+'dB,'+ 'FL'+ ' Model ACC reaches %.3f %%' %  (max(Acc_bfMerge))  )\n","        plt.legend(loc='lower right')\n","        plt.show()\n","\n","        plt.figure(2,figsize=(5, 4), dpi=80)\n","        l1, = plt.plot( Acc_tol, color='blue',label='Acc after merge')\n","        l2, = plt.plot( Acc_bfMerge, color='red', label='Acc before merge')\n","        plt.legend(loc='lower right')\n","        plt.title('SNR='+str(SNR)+'dB,'+ 'FL' + ' Model ACC reaches %.3f %%(after merge) and %.3f %%(before merge)' %  (max(Acc_tol), max(Acc_bfMerge)) )\n","        plt.show()        \n","\n","        time_end=time.time()\n","        print('1 epoch time cost:',time_end-time_start,'s')\n","\n","    df1 = pd.DataFrame()\n","    # df1['acc_old'] = xx\n","    df1['Accuracy aft Merg'] = Acc_tol\n","    df1['Accuracy before Merg'] = Acc_bfMerge\n","    df1['FA'] = Acc_PFA\n","    df1['PD'] = Acc_PD\n","    # print('test loaded best models',testnets4FL( net_list, class_dir, tol_test_loader ))\n","\n","    with pd.ExcelWriter(address_model + \"converg\"+\"SNR\"+str(SNR)+\".xlsx\", mode='w') as writer:  #mode was 'a'\n","        df1.to_excel(writer, sheet_name='FL')\n","    print('statics saved to excel:', address_model + \"converg\"+\"SNR\"+str(SNR)+\".xlsx\")\n","\n","    '''ROC module of current standalone model, saved in pd2 and pfa2'''\n","    #ROC ==========================================================================================\n","    pd2= []\n","    pfa2 = []\n","\n","    if best_acc < best_acc_bfMerge: #if bf_merge is better\n","        local_state = torch.load(address_model+'bestmodel/'+name0+ 'local_nodes'+'.pth')\n","        for i in range(len(net_list)):\n","            net_list[i].load_state_dict(local_state['net_dict_list'][i]) \n","    else: #if merged is better\n","        Merged_state = torch.load(address_model+'bestmodel/'+name0+ 'merged_nodes'+'.pth')\n","        net_tol.load_state_dict( Merged_state['net_tol'])\n","        net_list = Split_FL(net_list, net_tol, idx_list, class_dir)\n","\n","    for thresh_val in [ (i+.5) / roc_dots for i in range(roc_dots)]:\n","        print('threshold:', thresh_val)\n","        CNNoutput = testnetsVote(net_list, class_dir_real, tol_test_loader, coef_list, gain_dif, thresh_sig=thresh_val ) #\n","        pd2.append(CNNoutput[1].to(torch.device('cpu')).item())\n","        pfa2.append(CNNoutput[2].to(torch.device('cpu')).item())\n","\n","    plt.title(\"ROC of \" +'FL'+ \" method in SNR=\"+str(SNR)+\"dB\")\n","    l2, = plt.plot(pfa2, pd2, color='green', label='Transformer')\n","    plt.legend(loc='lower right')\n","    plt.show()\n","\n","    dfroc = pd.DataFrame() # save statics to excel\n","    # df1['acc_old'] = xx\n","    \n","    dfroc['PFA'] = pfa2\n","    dfroc['PD'] = pd2\n","    with pd.ExcelWriter(address_model + \"ROC_SNR\"+str(SNR)+\".xlsx\", mode='w') as writer:  #mode was 'a'\n","      dfroc.to_excel(writer, sheet_name='FL')\n","    print('ROC in Excel saved to:', address_model + \"ROC_SNR\"+str(SNR)+\".xlsx\")\n","\n","    ROC_dict = {\n","        'pd':pd2,\n","        'pfa':pfa2,\n","    }\n","    torch.save(ROC_dict, address_model+'FL'+'ROC.pth')\n","    print('ROC in Lists saved to:', address_model+'FL'+'ROC.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZRkjN_XEO2t"},"outputs":[],"source":["from torchsummary import summary \n","# v = AlexNet1D(num_classes = 10).to(device)\n","summary(net_list[0], (1,64,20))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gc\n","gc.collect()\n"]}],"metadata":{"accelerator":"GPU","celltoolbar":"Tags","colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
