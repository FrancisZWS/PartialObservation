{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vm9Xe9JjFihS"},"outputs":[],"source":["'''\n","2024 Feb version. model pre-trained on normal 2xdata, tested with PUs using Random Mod type '''\n","import torch\n","import os\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","import math\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.utils.data.distributed as TUDdistributed\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from torch.autograd import Variable\n","import sys\n","from copy import deepcopy\n","import pandas as pd\n","import random\n","import collections\n","#from MobileNetV1_CriticalPath import Net\n","import matplotlib.pyplot as plt\n","# from sklearn.externals import joblib\n","from cnn_models import standalone_cnn\n","from cnn_models import decouple_cnn\n","\n","from pytz import timezone\n","TMZ = timezone('EST')\n","import datetime\n","import shutil\n","import time\n","\n","device = torch.device(\"cuda\")\n","use_cuda = True\n","criterion = nn.CrossEntropyLoss()\n","\n","random.seed(0)\n","\n","        \n","\n","def create_net(chn_list, per_class_filter, ty_chs):\n","    number_class = len(chn_list) # tol_class num is 3\n","    d_w = number_class*per_class_filter\n","    cfg = [40, d_w, d_w, d_w ]\n","    print('corresponding cfg channel list:',cfg)\n","    # net = ensemble_vgg(dataset=number_class, cfg = cfg) \n","    net = standalone_cnn(nch = number_class, cfg=cfg, ty_chs=ty_chs)\n","    net.to(device)\n","    return net\n","\n","def testnetsVote(model_list, class_dir, test_loader, coef_list, gain_dif, thresh_sig = 0.5):                             \n","    '''default for FL CEL stdaln, Tests all nodes together, apply majority vote for each band \n","    '''\n","    total = 0 #sum of occupation and emptiness\n","    test_loss = 0 \n","    total_ocp = 0 #occupied bands\n","    total_emp = 0 #empty bands\n","    correct = 0\n","    total_cmb = 0  #occupation combination\n","    correct_ocp = 0\n","    correct_emp = 0\n","    correct_cmb = 0\n","    thresh_logit = -1*(math.log(thresh_sig**(-1) -1))\n","    # thresh_logit = thresh_sig\n","    \n","    with torch.no_grad():\n","        criterion = nn.BCEWithLogitsLoss()\n","        for batchidx, (data, target) in enumerate(test_loader):\n","            target =  target.cuda()\n","            #print(target.size())\n","            target = Variable(target)\n","            target = Variable(target)\n","#             print('target size is:',target.size())\n","            output_manual = torch.zeros(target.shape) # manual global decision, float version\n","            total_batch = float(torch.tensor(target.size()).prod()) #total num of channels in this batch\n","            total += total_batch\n","            total_cmb += float(target.size(0))\n","            total_ocp += float(target.sum())\n","            total_emp += float(total_batch - target.sum()) \n","            if (1-target).sum() != (torch.tensor(target.size()).prod() - target.sum()) : #Testing tensor dim =========\n","                print('show difference:',(1-target).sum(), total - target.sum())\n","                print('total ocp/emp calculation wrong')\n","            '''Above calculate the total number of channels detected and the ground truth number of occupation/emptiness'''\n","            coef = torch.tensor(coef_list)\n","            for idx in range(len(class_dir)): # appply fusion strategy on local_node detections\n","                model_list[idx].eval()\n","                datain = Variable(gain_dif*1e7*data[idx]).cuda() #1e7 to boost gradient\n","                localout = (model_list[idx](datain).cpu()>thresh_logit).float()\n","                output_manual[:,class_dir[idx]] = output_manual[:,class_dir[idx]].add_(localout) #add local detection result\n","                # output_manual[:,class_dir[idx]] = output_manual[:,class_dir[idx]]|(model_list[idx](datain).cpu()>thresh_logit)\n","            out_WOnorm = output_manual\n","            output_manual = output_manual.div_( coef ) > 0.49 #get fusion decision\n","            if batchidx == -1 :\n","                id = 90\n","                print('label:',target[id])\n","                print('un-normalized fusion:', torch.norm(out_WOnorm))\n","                print('SU fusion:',output_manual[id])\n","\n","            \n","\n","            correct_ocp += (output_manual*target.cpu()).sum()\n","            correct_emp += ((~output_manual)*(1-target.cpu())).sum()\n","            correct_cmb += (~(output_manual^( target.bool().cpu() ))).float().prod(1).sum() #use XNOR which is 'not+XOR'\n","\n","            loss = criterion(output_manual.float().cuda(), target)\n","            test_loss += loss.item()\n","            # print(output, loss.item())\n","\n","        correct_tol = deepcopy(correct_ocp) + deepcopy(correct_emp) #just sum\n","        accuracy_tol = 100*correct_tol/total\n","        accuracy_pd = 100*correct_ocp/total_ocp\n","        accuracy_pfa = 100*correct_emp/total_emp\n","        accuracy_cmb = 100*correct_cmb/total_cmb\n","\n","    loss = loss.item()\n","    print('test last batch',(~(output_manual^target.bool().cpu())).float().prod(1).sum() / target.size(0))\n","    print('++++++++++++++++++ Accuracy on global set: total: %d %%, combination: %d %%, PD: %d %%, PFA: %d %%, loss: %.3f' \\\n","                                            % ( accuracy_tol, accuracy_cmb, accuracy_pd, 100-accuracy_pfa, loss))\n","    return (accuracy_tol, accuracy_pd, 100-accuracy_pfa)\n","\n","\n","def shared_ch(class_dir, idx):\n","    '''idx-th local dataset, find the shared channel with each nearby local models(idx-1 and idx+1) \n","    and the corresponding ouput logit position at nearby models'''\n","    out_chn_idx = []#list of tuples\n","    #the channel each neighbor detects and the model output of each neighbor for this channel\n","    for neighbor in [class_dir[idx-1], class_dir[(idx+1)%len(class_dir)]]:\n","        for ch in class_dir[idx]:  \n","            if ch in neighbor:\n","                out_chn_idx.append((ch, neighbor.index(ch))) #should be only one for each neighbor\n","    \n","    return out_chn_idx\n","\n","\n","def adjust_learning_rate(optimizer, epoch):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n","    epoch_inter = 30\n","    bs = 0.2\n","    # bs = 1\n","    lr = 0.05 * bs ** (epoch//epoch_inter)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","def train(model, epoch, class_list, train_loader, thresh_sig = 0.5):\n","    '''Use NodeDatasetMaker by default, trainloader only gives local data and only trains a local model'''\n","    # optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n","    # optimizer = optim.Adam(model.parameters(), lr=0.1, weight_decay=1e-4)\n","    optimizer = optim.Adam(model.parameters(), lr=0.1, weight_decay=1e-5) # use ADAM for standalone\n","    adjust_learning_rate(optimizer, epoch)\n","#     print(\"\\nLocal Epoch\", epoch)\n","    sys.stdout.flush()\n","\n","    for param_group in optimizer.param_groups:\n","        print('Learning Rate: %f' % param_group['lr'])\n","    # sys.stdout.flush()\n","    model.train()\n","    thresh_logit = -1*(math.log(thresh_sig**(-1) -1))\n","#     print('threshold on output logits', thresh_logit)\n","    train_loss = 0\n","    criterion = nn.BCEWithLogitsLoss()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.cuda(), target.cuda()     \n","        #print( data.size() )\n","        data, target = Variable(1e7*data), Variable(target)\n","\n","        optimizer.zero_grad()\n","        #output = model(data, len(class_list))\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()#remains to be edited\n","    return model\n","\n","def gather_layer(model): # Gather layers of a model to 3 categories: convlist, batchnormlist, fclist\n","    convlist = []\n","    batchnormlist= []\n","    fclist =[]\n","    for layer in model.features:\n","        if isinstance(layer, nn.Conv2d):\n","            convlist.append(layer)\n","        if isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.GroupNorm):\n","            batchnormlist.append(layer)\n","    fclist.append(model.fc)\n","    return convlist, batchnormlist, fclist\n","\n","\n","\n","\"\"\"626/720 dataset maker, each logit corresponds to the occupation of a single channel\"\"\"\n","class TotalDatasetMaker(Dataset):\n","    \"simple version that requires the user to edit input/label format elsewhere\"\n","    def __init__(self, db, label_list, transformFunc ):\n","        \"\"\"\n","        db: a list of input signal tensors, label_list: a list of data labels, corresponding to db.\n","        \"\"\"\n","        self.datasets = db\n","        self.label_list = label_list\n","        self.transformFunc = transformFunc\n","    def __getitem__(self, i):\n","        img = self.datasets[i]\n","        img = self.transformFunc(img)\n","        class_label = self.label_list[i]\n","        return img, class_label\n","\n","    def __len__(self):\n","        return len(self.label_list)\n","    \n","    \n","\"\"\"720 dataset maker, data looks like: each global channel occupation condition ==> \n","each node only learn from 'local' received signal(full size for each node) which is an element in the list of this condition \"\"\"\n","class NodeDatasetMaker(Dataset):\n","\n","    def __init__(self, db, label_list, node, class_dir, transformFunc ):\n","        \"\"\"\n","        db: a list of input signal tensors, label_list: a list of data labels, corresponding to db.\n","        node\n","        \"\"\"\n","        self.datasets = db\n","        self.label_list = label_list\n","        self.transformFunc = transformFunc\n","        self.chn_list = class_dir[node]\n","        self.node = node\n","    def __getitem__(self, i):\n","        img = self.datasets[i][self.node]\n","        img = self.transformFunc(img)\n","        class_label = self.label_list[i][self.chn_list]\n","        return img, class_label\n","\n","    def __len__(self):\n","        return len(self.label_list)\n","    \n","def Dis_analysis(class_dir, tol_list):\n","  #idx_list: list of lists, global positions of locally observable bands for each node\n","  #coef_list: coef for averaging the param for each band, how many nodes are learning each certain band\n","    idx_list = []\n","    coef_list = [0]*len( tol_list )\n","    for i in range( len(class_dir) ): #Generating the mapping btw nodes and net_tol\n","        sub_idx_list = [] \n","        for j in class_dir[i]:\n","            for k in range( len(tol_list) ):\n","                if j == tol_list[k]:\n","                    sub_idx_list.append(k)\n","                    coef_list[k] += 1 \n","                    break\n","        idx_list.append(sub_idx_list)\n","    return idx_list, coef_list\n","\n","def setDir(filepath):\n","  # if directory not exist, create. if directory already exist, empty it.\n","  if not os.path.exists(filepath):\n","    os.makedirs(filepath)\n","  else:\n","    print('Directory already exists')\n","    shutil.rmtree(filepath, ignore_errors = True)\n","    os.mkdir(filepath)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"15lH-nuXAInZOUJH21U5yBgGuPjnZnyiD"},"executionInfo":{"elapsed":6056744,"status":"error","timestamp":1684273712335,"user":{"displayName":"Weishan Zhang","userId":"02899996824368835811"},"user_tz":240},"id":"SEb7xE2RFihd","outputId":"907aff10-71df-4d34-a613-37238caa08dc"},"outputs":[],"source":["for snr in [14]: # [2, 4, 6, 8, 10, 12, 14, 16] [ 4, 12, 14] [8, 14, 16, 20, 6 ]: #[12, 8, 14, 16, 20]: #[12, 8 ]: #[12, 8, 14, 16, 20]\n","\n","    # Load data =========================================================================================================\n","    roc_dots = 100\n","    nepoch = 0\n","    volum = 20\n","    SNR= -1*snr\n","    DistAmp_tr = 10 # DistAmp = 10 #25\n","    DistAmp_te = 10 #25    \n","    alpha_tr = 3.71\n","    alpha_te = 3.71\n","    gain_dif= (DistAmp_te*2*3**0.5 /3)**alpha_te / (DistAmp_tr*2*3**0.5 /3)**alpha_tr\n","    stage_dir='/SNRs/'#for naming and directory\n","    datadir = 'RefinedNewData/SNRs/RandMod/Data_SNR'+str(SNR)+'vol1'+'.pth' # RandMod: revision dataset, PU mod varying (testing data only)\n","    datadir_te = datadir\n","    datadir_tr = datadir\n","    Model_dir = 'Saved_Models/standalone_cnn/LoadedModel/'+str(SNR)+'dBVol20'\n","\n","    data_dict_tr = torch.load(datadir)\n","    data_dict_te = torch.load(datadir)\n","    data_dict_tr.keys()\n","\n","    db = data_dict_tr['training data list']\n","    label_list = data_dict_tr['training label list']\n","\n","    db_te = data_dict_te['testing data list']\n","    label_list_te = data_dict_te['testing label list']\n","\n","    # Create CNN =========================================================================================================\n","\n","    per_class_filter = 8\n","    shared_layers = 1 # not used\n","    class_dir=[[0, 3, 4, 19], [0, 1, 10, 4, 19], [1, 10, 4, 19, 5, 13], [1, 10, 2, 11, 14, 5, 13], [2, 11, 14, 5, 13, 6, 15, 17], [3, 4, 19, 7, 12, 18], [4, 19, 7, 12, 18, 8, 16], [4, 19, 5, 13, 8, 16], [5, 13, 8, 16, 9], [5, 13, 6, 15, 17, 9]]\n","\n","    tol_list = []\n","    for classi in class_dir:\n","      tol_list += classi\n","    tol_list = list( set(tol_list) )\n","    tol_list.sort()\n","\n","    print(class_dir)\n","    print(tol_list)\n","\n","    idx_list, coef_list = Dis_analysis(class_dir, tol_list)\n","    print(idx_list, coef_list)\n","\n","    #Parepare nets\n","    net_list = []\n","    Acc_tol=[]\n","\n","    for i in range(len(class_dir)):\n","        net_list.append(create_net(class_dir[i], per_class_filter, True))\n","        # Acc_list.append([])\n","\n","    db_tr_list = []\n","    for idx in range(len(class_dir)):# train datasets are more complex\n","        db_tr_list.append(NodeDatasetMaker( db, label_list, idx, class_dir, transforms.Compose([ ]) ))\n","    db_te_1 = TotalDatasetMaker( db_te, label_list_te, transforms.Compose([ ]) )\n","\n","    train_loader_list = []\n","    for idx in range(len(class_dir)):# trainloaders are more complex\n","        train_loader_list.append(DataLoader(db_tr_list[idx], batch_size=100, shuffle=True, num_workers=4, pin_memory=True))\n","    tol_test_loader = DataLoader(db_te_1, batch_size=1024, shuffle=False, num_workers=4, pin_memory=True)\n","\n","    # aggre_inter = 1 #currently unused\n","\n","    now=datetime.datetime.now(TMZ) #time watermark\n","    time_watermark = now.strftime('%y%m%d_%H_%M')\n","    print('model watermark',time_watermark)\n","    address_model = 'Saved_Models/'+type(net_list[0]).__name__+stage_dir+str(SNR)+'dBVol'+str(volum)+'_'+time_watermark+'/' #root dir for saved models\n","    setDir(address_model+'checkpoint/') # if dir not exist, create. if dir already exist, empty it.\n","    setDir(address_model+'bestmodel/')\n","    print('Models saved to dir:\\n', address_model)\n","    name0 = type(net_list[0]).__name__ +'_SNR'+str(SNR)+'vol'+str(volum) # common part of DNN node names\n","\n","    txt=open(address_model+'Datasetdir.txt',\"w\").write(datadir_te) #save dataset dir (dataset version)\n","\n","    # Train =========================================================================================================\n","    Acc_PD = []\n","    Acc_PFA = []\n","    Acc_cmb = []\n","    Acc_tol = []\n","    plt.title(\"Global Model ACC of the proposed method\")\n","    best_acc = 0\n","    Acc_tol.append( testnetsVote( net_list, class_dir, tol_test_loader, coef_list, gain_dif, thresh_sig = 0.5 )[0].item() )\n","    for epoch in range(nepoch):\n","        time_start = time.time()\n","        # Train & save dicts of n1 n2\n","        print('epoch:',epoch)\n","        for i in range(len(net_list)):\n","            net_list[i] = train(net_list[i], epoch, class_dir[i], train_loader_list[i])\n","            name1 = address_model+'checkpoint/'+name0+ '_node' +str(i)+ '.pth'\n","            torch.save(net_list[i].state_dict(), name1)\n","        best_acc = max(Acc_tol)\n","        Acc_tol.append( testnetsVote(net_list, class_dir, tol_test_loader, coef_list, gain_dif, thresh_sig = 0.5)[0].item() )\n","        print('Saving..')\n","        state = {\n","            'net': [net.state_dict() for net in net_list],\n","            'acc': Acc_tol[-1],\n","            'epoch': epoch,\n","        }\n","        if Acc_tol[-1] > best_acc:\n","            # torch.save(state, './bestmodel/standalone135_Jan2022.pth')\n","            for i in range(len(net_list)):\n","                name1 = address_model+'bestmodel/'+name0+ '_node' +str(i)+ '.pth'\n","                torch.save(net_list[i].state_dict(), name1)\n","            best_acc = 1*Acc_tol[-1]\n","\n","        plt.figure(1,figsize=(5, 4), dpi=80)\n","        l1, = plt.plot( Acc_tol, color='blue', label='Avg Acc/band')\n","        l2, = plt.plot( Acc_PFA, color='red', label='Acc 4 empty')\n","        l3, = plt.plot( Acc_PD, color='black', label='Acc 4 busy')\n","        plt.title('SNR='+str(SNR)+'dB,'+ type(net_list[0]).__name__+ ' Model ACC reaches %.3f %%' %  (max(Acc_tol))  )\n","        plt.legend(loc='lower right')\n","        plt.show()\n","\n","        plt.figure(2,figsize=(5, 4), dpi=80)\n","        l1, = plt.plot( Acc_tol, color='blue',label='Avg Acc/band')\n","        plt.legend(loc='lower right')\n","        plt.title('SNR='+str(SNR)+'dB,'+ type(net_list[0]).__name__+ ' Model ACC reaches %.3f %%' %  (max(Acc_tol))  )\n","        plt.show()\n","        \n","        time_end=time.time()\n","        print('1 epoch time cost:',time_end-time_start,'s')\n","\n","    for i in range(len(net_list)):\n","        net_list[i].load_state_dict(torch.load(Model_dir+'/bestmodel/'+name0+'_node'+str(i)+'.pth'))\n","    print('best models loaded')\n","\n","    df1 = pd.DataFrame()\n","    df1['Accuracy'] = Acc_tol\n","\n","    with pd.ExcelWriter(address_model + \"converg\"+\"SNR\"+str(SNR)+\".xlsx\", mode='w') as writer:  #mode was 'a'\n","        df1.to_excel(writer, sheet_name=type(net_list[0]).__name__)\n","    print('statics saved to excel:', address_model + \"converg\"+\"SNR\"+str(SNR)+\".xlsx\")\n","\n","    # ROC =========================================================================================================\n","    '''ROC module of current standalone model, saved in pd2 and pfa2'''\n","    pd2= []\n","    pfa2 = []\n","\n","    for thresh_val in [ (i+.99999)/200 for i in range(200)]:\n","        print('threshold:', thresh_val)\n","        CNNoutput = testnetsVote(net_list, class_dir, tol_test_loader, coef_list, gain_dif, thresh_sig=thresh_val) #\n","        pd2.append(CNNoutput[1].to(torch.device('cpu')).item())\n","        pfa2.append(CNNoutput[2].to(torch.device('cpu')).item())\n","\n","    plt.title(\"ROC of \" +type(net_list[0]).__name__+ \" method in SNR=\"+str(SNR)+\"dB\")\n","    l2, = plt.plot(pfa2, pd2, color='green', label='Transformer')\n","    plt.legend(loc='lower right')\n","    plt.show()\n","\n","    dfroc = pd.DataFrame() # save statics to excel\n","    dfroc['PFA'] = pfa2\n","    dfroc['PD'] = pd2\n","    \n","    with pd.ExcelWriter(address_model + \"ROC_SNR\"+str(SNR)+\".xlsx\", mode='w') as writer:  #mode was 'a'\n","      dfroc.to_excel(writer, sheet_name=type(net_list[0]).__name__)\n","    print('ROC in Excel saved to:', address_model + \"ROC_SNR\"+str(SNR)+\".xlsx\")\n","\n","    ROC_dict = {\n","        'pd':pd2,\n","        'pfa':pfa2,\n","    }\n","    torch.save(ROC_dict, address_model+type(net_list[0]).__name__+'ROC.pth')\n","    print('ROC in Lists saved to:', address_model+type(net_list[0]).__name__+'ROC.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQcLh5w2EcQT"},"outputs":[],"source":["from torchsummary import summary \n","# v = AlexNet1D(num_classes = 10).to(device)\n","summary(net_list[0], (1,64,20))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","\n","# To stop the kernel using sys.exit()\n","sys.exit()\n","\n","# To stop the kernel using KeyboardInterrupt\n","raise KeyboardInterrupt"]}],"metadata":{"accelerator":"GPU","celltoolbar":"Tags","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
